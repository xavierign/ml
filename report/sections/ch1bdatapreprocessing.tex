\chapter{Data Preprocessing}
We started by analyzing the feature set of the dataset. It is a collection of numerical, categorical and binary variables.
Given that the algorithms that we wanted to use only considered numerical or binary variables, we decided to transform categorical data to dummy variables in both the data and quiz file.

We noticed that some categories had a very low frequency. Hence, we grouped these categories in a new category called \textit{other}. We chose so in order to simplify our feature map and to avoid outliers.  (first hyperparameter min number of obs in a particular category -- What?) 

To further simplify and optimize our dataset we transformed variables that took only two values to binary. One such example is the variable called \textit{Variable}.

We anticipated that the decision boundary may be non-linear in $x$ but could be linear in $\phi(x)$.
To manage this effect, we tried two different feature maps.


\section{Feature Maps}
We tried three feature maps, of which we only kept 2 in the final implementation
\subsection{f2 complete map}
A complete \textit{order-2} feature map, considering all degree-2 and degree-1 terms. Let 
$\phi : \mathbb{R}^d \rightarrow \mathbb{R}^D$ be the expanded feature mapping given by 
\begin{center}
$\phi(x) := (x_1, x_2, ... , x_d, x_{2,1}, x_{2,2}, ... , x_{2,d}, x_1x_2, x_1x_3, ... , x_1x_d, ... ,x_{d-1}x_d)$
 \end{center}
 
We initially tried this feature map on our algorithms but, unfortunately, it did not yield any improvements which, added to its significant computational cost and the complexity it added to the model, was reason enough for us to discard it.

\subsection{f2 strict map}
 This feature map converts a feature set into a \textit{strictly-order-2} map. That is, only interaction terms between different features are considered. Let $\phi : \mathbb{R}^d -> \mathbb{R}^D$ be the expanded feature mapping given by \newline
 \begin{center}
 $\phi(x) := (x_{1,2}, x_{1,3}, ... , x_{1,d}; x_{2,1}, x_{2,3}, ... , x_{2,d}, ... ,x_{d-1,d}) $
 \end{center}

\subsection{fix2 map} 
This map replaces low-frequency occurrences of the value $'2'$ in categorical features taking values $'0','1'$ and $'2'$, and adds an extra column stating whether a $'2'$ value was replaced or not. This is done to improve the efficiency of the models by replacing one ternary variable with two binary variables. 


\section{Implementation of the Feature Maps}

\begin{itemize}
\item Initially, we fitted a \textit{TreeBagger} ensemble model once on the dataset in order to detect the predictors with the highest importance. Then, we extended the dataset is extended by applying the \textit{f2 strict} map to these predictors. However, since this approach yielded no performance improvements, we discarded this option.
\item After discarding the previous step, we extended the dataset again by applying the \textit{f2 strict} map applied on all numerical variables.
\item Finally, we applied the \textit{fix2} map on the categorical variables where it applied.
\end{itemize}


%%\begin{example}{An Example of an Example}
%%  \label{ex:simple_example}
%%  Here is an example with some math
%%  \begin{equation}
%%    0 = \exp(i\pi)+1\ .
%%  \end{equation}
%%  You can adjust the colour and the line width in the {\tt macros.tex} file.
%% \end{example}



%\paragraph{A Paragraph}
%You can also use paragraph titles which look like this.

%\subparagraph{A Subparagraph} Moreover, you can also use subparagraph titles which look like this\todo{Is %it possible to add a subsubparagraph?}. They have a small indentation as opposed to the paragraph titles.

%\todo[inline,color=green]{I think that a summary of this exciting chapter should be added.}