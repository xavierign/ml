\chapter{Models considered}
\label{ch:ch2label}
Random forests is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. In this application the algorithm was used as a classifier. Random decision forests correct for decision trees' habit of overfitting to their training set. The algorithm for inducing Breiman's random forest was developed by Leo Breiman \citep{breiman2001random}.
 
TreeBagger is the matlab implementation of the Random Forest algorithm. 
Bagging stands for bootstrap aggregation. Every tree in the ensemble is grown on an independently drawn bootstrap replica of input data. Observations not included in this replica are "out of bag" for this tree. To compute prediction of an ensemble of trees for unseen data, TreeBagger takes an average of predictions from individual trees. To estimate the prediction error of the bagged ensemble, you can compute predictions for each tree on its out-of-bag observations, average these predictions over the entire ensemble for each observation and then compare the predicted out-of-bag response with the true value at this observation \citep{treebagger}.

TreeBagger relies on the Classification Tree functionality for growing individual trees. In particular, Classification Tree accepts the number of features selected at random for each decision split as an optional input argument.

The most significant parameters the algorithm takes and their optimal values are listed as following. 

\textbf{MinLeafSize}: Minimum number of observations per tree leaf. We tested these values: 1, 2, 5, 10. The optimal value obtained was 5.

\textbf{NumTrees}: Scalar value equal to the number of decision. This parameters counts as the number of iterations that the algorithm performs. In theory, and verified with the results, both training and testing error does not increase with the number of iterations as the overfitting is avoided. The values tested were 5-50-100-150-200-250-300-350-500. The best results were obtained with the highest numbers of trees.

\textbf{NumPredictorsToSample}: Number of predictor or feature variables to select at random for each decision split. This is a very relevant parameters to tune. The value tested range from 10 to 200. The optimal value obtained was at 100.
